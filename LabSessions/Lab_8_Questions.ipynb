{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Lab 8 - Explainability in Data Analytics and Machine Learning\n",
    "Week 8 - Q3, 22/23 <br>\n",
    "SEN163B: Responsible Data Analytics<br>\n",
    " "
   ],
   "metadata": {
    "id": "y6J2EwOzZlvc"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "By <b> Nadia Metoui </b> <br>\n",
    "TA <b> Anagha Magadi Rajeev</b> <br> \n",
    "Faculty of Technology, Policy, and Management (TPM)<br>"
   ],
   "metadata": {
    "id": "cfGpyrvWOylt"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "***Learning Objectives***<br>\n",
    "At the end of this lab, you will be able to \n",
    "- Implement and Examine Intrinsic Explainable ML Models. <br>\n",
    "- Implement and Examine Post-Hoc Explainability  Models. <br>\n",
    "- Discuss explainability in a Multi-stakeholders Socio-Technical context.\n"
   ],
   "metadata": {
    "id": "IoaomgbZf3Jy"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "***Structure***\n",
    "- Part 0. Setting up the Lab.\n",
    "- Part I. Intrinsic Explainability\n",
    "- Part II. LIME: Post-Hoc Explainability (LIME Local + SP-LIME Global)\n",
    "- Part III: SHAP: Post-Hoc Explainability (Local, Global, Cohort)"
   ],
   "metadata": {
    "id": "6dnnUZUxtaDG"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 0. Setting up the Lab:\n",
    "Step 1: Install and Load the libraries for the Lab.<br>\n",
    "Step 2: Load and Prepare the Data.<br>\n",
    "Step 3: Familiarize with the dataset <br>\n",
    "Step 4: Train the ML model <br>"
   ],
   "metadata": {
    "id": "URLk3fb3hKp5"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###Step 1: Install and Load the libraries for the Lab.\n",
    "\n"
   ],
   "metadata": {
    "id": "-uIcvkRZj6RK"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "R6fakJnbkQeG"
   },
   "source": [
    "!pip install lime\n",
    "!pip install shap"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VqHibpnYkQeG",
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sklearn.model_selection\n",
    "import sklearn.metrics\n",
    "import sklearn.datasets\n",
    "import sklearn.ensemble\n",
    "import sklearn.preprocessing\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from IPython.display import Markdown, display\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "from lime import submodular_pick\n",
    "import shap\n",
    "from shap.plots import _waterfall\n",
    "import xgboost\n",
    "from xgboost import plot_importance\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(1)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "###Step 2: Load and Prepare the Data.\n"
   ],
   "metadata": {
    "id": "gsjZmdvvkFXw"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LhM5lhswq9IM"
   },
   "source": [
    "We will be using the **Adult Dataset** ([full documentation here](https://archive.ics.uci.edu/ml/datasets/adult)), which is an \"Census Income\" dataset (USA). We will use this dataset predict whether a person makes over 50K dollars per year based on census information (e.g., age, education, country of birth etc.). We will be using the version of the dataset hosted online [here](https://archive.ics.uci.edu/ml/datasets/adult)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KV9s8V2iq9IO"
   },
   "source": [
    "#### Load the dataset\n",
    "We will download the dataset directly from the online archive.<br>\n",
    "Name the columns. And store the data in a dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Mx0PKgRUq9IO"
   },
   "source": [
    "# Name Columns\n",
    "colnames = [\"Age\", \"Workclass\", \"fnlwgt\", \"Education\", \"Education-Num\",\n",
    "            \"Marital-Status\", \"Occupation\", \"Relationship\", \"Race\", \"Sex\",\n",
    "            \"Capital-Gain\", \"Capital-Loss\",\"Hours-per-week\", \"Country\",\n",
    "            \"income\"]\n",
    "\n",
    "# Load in the data\n",
    "# If you have any issues downloadting the data replace the link by the adult.data file\n",
    "data_df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data',\n",
    "                      names = colnames)\n",
    "print(\"Shape: \", data_df.shape)\n",
    "data_df.head()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "buhCTvcuq9IR"
   },
   "source": [
    "# Drop Education-Num, which has a functional dependency with Education\n",
    "# Same Tuple Values\n",
    "print(\"Show functional dependency\")\n",
    "print(data_df.groupby(['Education','Education-Num']).size())\n",
    "data_df.drop(['Education-Num'], axis=1, inplace=True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Drop fnlwgt might not be the best choice but I could not figutr out what this attribut stands for\n",
    "# Better safe than sorry ;-)\n",
    "data_df.drop(['fnlwgt'], axis=1, inplace=True)"
   ],
   "metadata": {
    "id": "BYFrKX0TFnuI"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "RpadUNmNq9IT"
   },
   "source": [
    "# Get a list of feature names (excluding the outcome variable: income)\n",
    "feature_names = data_df.columns[:-1]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "RNtzudwOq9IT"
   },
   "source": [
    "# Mark labels and encode them using sklearn\n",
    "labels = data_df.iloc[:,-1]\n",
    "le= sklearn.preprocessing.LabelEncoder()\n",
    "le.fit(labels)\n",
    "labels = le.transform(labels)\n",
    "class_names = le.classes_\n",
    "data = data_df.iloc[:,:-1]\n",
    "le_label_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print(\"Class names: \", class_names)\n",
    "print(\"Label mapping: \", le_label_mapping)\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7w56XCNeq9IU"
   },
   "source": [
    "# Check if there are categorical varibles that we need to make dummies for\n",
    "print(data.dtypes)\n",
    "# Get a list of which variables are categorical\n",
    "categorical_features  = [i for i in range(len(data.dtypes)) if data.dtypes[i]=='object']\n",
    "print(\"Indices of categorical features: \", categorical_features)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "###Step 3: Familiarize with the dataset. \n"
   ],
   "metadata": {
    "id": "O_XwVHPwkFjT"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"Shape: \", data_df.shape)\n",
    "data_df.head()"
   ],
   "metadata": {
    "id": "I4a_hCJlnlad"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "###Step 4: Train the ML model "
   ],
   "metadata": {
    "id": "R0ceuQHZkF0u"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nLCXpuzhq9IW"
   },
   "source": [
    "####Feature Preparation (Compatible with LIME for Part I and II)\n",
    "**Note:** in PART II. LIME will require us to provde categorical variables as a single column, not as dummies, so we can't just explode these columns the way we normally would during pre-processing.\n",
    "\n",
    "To avoid redoing this part for Intrensic and Post-Hoc explanability we will use some sklearn tools and do the following:\n",
    "\n",
    "1. Encode the existing categories with a number corresponding to each category\n",
    "2. Make a dictionary storing the relationship between the original stirng category and the number we've replaced it with (categorical_names)\n",
    "3. Make a function that we can use down the line to transform categorical variables into dummies"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eetKPh7hq9IW"
   },
   "source": [
    "categorical_names = {}\n",
    "for feature in categorical_features:\n",
    "    print(\"Feature: \", feature)\n",
    "    # Use label encoder to map categories to numbers\n",
    "    le = sklearn.preprocessing.LabelEncoder()\n",
    "    le.fit(data.iloc[:, feature])\n",
    "    # Replace the categories with corresponding numbers in the original data\n",
    "    data.iloc[:, feature] = le.transform(data.iloc[:, feature])\n",
    "    # Store and print the mappings for reference later\n",
    "    categorical_names[feature] = le.classes_\n",
    "    print(categorical_names[feature])\n",
    "    print(\"==================================================\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "K9AZ7Yhgq9IX"
   },
   "source": [
    "# This variable is where we store the original names of each category for each variable\n",
    "categorical_names"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YmXlL0fvq9IZ"
   },
   "source": [
    "# We can (and will) use this encoder function to transform the categorical columns into dummies-- \n",
    "# but we can't do that to the original dataset if we want to use LIME After training\n",
    "encoder = ColumnTransformer(transformers=[('get_dummies', OneHotEncoder(), categorical_features)], remainder='passthrough')\n",
    "encoder = encoder.fit(data)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FUZI7wvOq9Ia"
   },
   "source": [
    "#### Split into training and test sets\n",
    "\n",
    "We will us an 80/20 train-test split. We won't be doing any hyperparameter tuning during this lab, so no need to worry about a validation set. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dorpzcSgq9Ib"
   },
   "source": [
    "train, test, labels_train, labels_test = sklearn.model_selection.train_test_split(data, labels, train_size=0.80, random_state=10)\n",
    "print(\"Train shape: \", train.shape)\n",
    "print(\"Test shape: \", test.shape)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qfD3plyiq9Ib"
   },
   "source": [
    "####Train the model\n",
    "\n",
    "We will be using gradient boosted decision trees. Gradient boosting machine learning methods such as XGBoost are state-of-the-art for these types of prediction problems with tabular style input data of many modalities. \n",
    "For this Lab we will user an implementation from the [xgboost](https://github.com/dmlc/xgboost) package."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qNp3U31xq9Ic"
   },
   "source": [
    "# Fit the model\n",
    "gbtree = xgboost.XGBClassifier(n_estimators=200, max_depth=5)\n",
    "gbtree.fit(encoder.transform(train), labels_train)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tGqRGKPWq9Id"
   },
   "source": [
    "# Make predictions \n",
    "pred_labels_test = gbtree.predict(encoder.transform(test))\n",
    "\n",
    "# Calcualte accuracy on the test set\n",
    "print(\"Test set accuracy: \", sklearn.metrics.accuracy_score(labels_test, pred_labels_test))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part I: Intrinsic Explainability\n",
    "As explained in class some ML models (usually less complex ones) have ability to provide some explanations about decisions these models make without the need for an \"external\" explainer. Tree-based ML models can provide information about feature importance for the predictions.\n",
    "\n",
    "In the following we will: \n",
    "- Produce measur(s) of feature importance for our gradient boosted decision tree.\n",
    "- Take a closer look at thes intrensic explanations and discuss the results."
   ],
   "metadata": {
    "id": "fOjlbMxXiFGk"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tUV4Lpp9q9Ig"
   },
   "source": [
    "#### Step 1. Feature importance decision trees (Using Weight)\n",
    "\n",
    "There are three options (or measurement creteria) for measuring feature importance in XGBoost* (and generally most tree-based models):\n",
    "\n",
    "1. **Weight:** The number of times a feature is used to split the data across all trees.\n",
    "2. **Cover:** The number of times a feature is used to split the data across all trees weighted by the number of training data points that go through those splits.\n",
    "3. **Gain:**. The average training loss reduction gained when using a feature for splitting.\n",
    "\n",
    "<small>* Note: A measure of feature importance is calculated for a single tree by counting how many splits occured on each variable (or measurs around a split see above). We calculate at a feature importance for the entire model by averaging the score for each feature across all trees in the forest. </small>\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Prepare for the feature importance plot\n",
    "# Get feature names for the transformed dataset so that they'll show up on the plot \n",
    "gbtree_features_orig = gbtree.get_booster().feature_names\n",
    "gbtree_features = []\n",
    "for cat_var in categorical_names:\n",
    "    cat_var_names = [feature_names[cat_var] +\" = \" + cat for cat in categorical_names[cat_var]]\n",
    "    gbtree_features.extend(cat_var_names)\n",
    "gbtree_features.extend([feature_names[i] for i in range(len(feature_names)) if i not in categorical_features])\n",
    "gbtree.get_booster().feature_names = gbtree_features"
   ],
   "metadata": {
    "id": "E4hk02E7Njvr"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (10,20)\n",
    "xgboost.plot_importance(gbtree, importance_type=\"weight\")\n",
    "plt.title(\"Feature Importance using weight as measurement creteria\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "id": "SJjkis_cApoJ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Question:**<br>\n",
    "1. What did we learn from this plot. How should we read it?\n",
    "2. What does it not tell us?\n",
    "3. Who can use these explanations? Which stakeholder?\n",
    "4. For what purpose(s)?"
   ],
   "metadata": {
    "id": "ZgAS_CKjLVJ6"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Answers**\n"
   ],
   "metadata": {
    "id": "lOpYQhrqIF1m"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pd-cRCaoq9Ii"
   },
   "source": [
    "#### Step 2. Feature importance decision trees (Change measurement creteria)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-j1NGR73q9I4"
   },
   "source": [
    "The plot_importance function takes a parameter called 'importance_type'(you can read more about [in the documentation](https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.plotting)). \n",
    "\n",
    "This parameter can take different values depending on the measurement you want to use (*Cover*, *Weight*, *Gain* etc see definitions for each of them above)\n",
    "\n",
    "Try changing the importance type and re-making the plot"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "r6xqC8xaq9Ii"
   },
   "source": [
    "# Your code here (for cove)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uMZtvdzXq9I5"
   },
   "source": [
    "# Your code here (for gain)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Questions:**<br>\n",
    "1. What did you notice using these different measurments of feature importance?\n",
    "2. What are the conseqences of that?"
   ],
   "metadata": {
    "id": "NK71SOyNJXKZ"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vwq_ZK35q9H5"
   },
   "source": [
    "## Part II: LIME: Post-Hoc Explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will explore both Local and Global LIME Explanations :\n",
    "- Step 1: LIME Local Explainer (for Tabular Data)\n",
    "- Step 2: SP-LIME Global Explainer (for Tabular Data)"
   ],
   "metadata": {
    "id": "8ZGtJMbcupXf"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "LIME: Locally Interpetable Model-agnostic Explanations:\n",
    "\n",
    "- Locally interpretable: for each specific prediction, we can provide a relevant explanation. \n",
    "- Model-agnostic: we can provide the same sort of explanation for models of different classes.\n",
    "- Can be used to produce Global Explanations (using SP-LIME)\n",
    "\n",
    "LIME has a python implementation package we will be using in this part. For more informations about the package see [full documentation here](https://github.com/marcotcr/lime) and [here](https://lime-ml.readthedocs.io/en/latest/index.html). The original paper describing the LIME method is [here](https://arxiv.org/pdf/1602.04938.pdf).\n"
   ],
   "metadata": {
    "id": "b9kQGMgDupl6"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7hov3P0uPx-O"
   },
   "source": [
    "### Step 1: LIME Local Explainer (for Tabular Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Replace the original feature names (to fit LIME expect format for features as discussed above)"
   ],
   "metadata": {
    "id": "Q5retcU-GmOM"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "m2Na9ju_q9I6"
   },
   "source": [
    "# Replace the original feature names, which LIME will expect\n",
    "gbtree.get_booster().feature_names = gbtree_features_orig"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then, we initialize an explainer object that takes in all the information we stored/encoded earlier about the dataset. "
   ],
   "metadata": {
    "id": "oQ7Qxw-BEEuG"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-6RMsMWsq9I7"
   },
   "source": [
    "\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(train.values, \n",
    "                                                   feature_names=feature_names,\n",
    "                                                   class_names=class_names,\n",
    "                                                   categorical_features=categorical_features, \n",
    "                                                   categorical_names=categorical_names)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_8ZuaqIq9I7"
   },
   "source": [
    "Then, we need to format data inputs and outputs for LIME (i.e., we need to define a single function that takes in the form of data that LIME expects and returns the type of prediction that LIME expects.) <br> \n",
    "\n",
    "In particular:\n",
    "- the input should be a numpy array (which we can get from a pandas df using .values)\n",
    "- the input features should be human-understandable\n",
    "- the input data should have each categorical variable in a single column\n",
    "- the output should be a predicted probability (not a predicted class) "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JygO8Hu7q9I8"
   },
   "source": [
    "predict_fn = lambda x: gbtree.predict_proba(encoder.transform(x)).astype(float)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CEwAQi8Zq9I9"
   },
   "source": [
    "Now we have everything we need to use the explainer. Let's get an explanation for one of the examples in the test set.\n",
    "\n",
    "The variable \"i\" indicates the data point for which we want an explanation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2Tn42gAtq9I-"
   },
   "source": [
    "i = 3\n",
    "print('Actual class: ', labels_test[i])\n",
    "# Get explanation\n",
    "exp = explainer.explain_instance((test.values[i]), predict_fn, num_features=5)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "J_Gq5gfTq9I_"
   },
   "source": [
    "# Visualize the explanation \n",
    "%matplotlib inline\n",
    "fig = exp.as_pyplot_figure()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hU6Z7Vchq9JA"
   },
   "source": [
    "The explanations can also be exported as an html page (which we can render here in this notebook), using D3.js to render graphs. You could also save the html page to a file if you wanted to."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YC2AmRIiq9JA"
   },
   "source": [
    "exp.show_in_notebook(show_all=True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3-e0HCinq9JJ"
   },
   "source": [
    "The explanation can also be presented as a list of weighted features."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TDedXxbZq9JK"
   },
   "source": [
    "exp.as_list()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now generate LIME explanations (in whichever format you chose) for some other examples in the test set. \n",
    "Change the value of \"i\" and explore some of the local explanations for other predictions"
   ],
   "metadata": {
    "id": "NiU15cJuHMLH"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Code here + add as many code cells as needed"
   ],
   "metadata": {
    "id": "bmrr3ElaHKzJ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aEwIFNwiq9JK"
   },
   "source": [
    "**Questions**\n",
    "\n",
    "1. What did we learn from this plot. How do we interpret this plot? \n",
    "2. What does it not tell us? what do red and green mean? What's on the x axis?\n",
    "3. How does this explantion differ from the feature intrinsic  importance explanation given above?\n",
    "4. When we changed the datapoint for which we what explanation, what happend? Are similar features important for all datapoints?\n",
    "5. Who can use these explanations? Which stakeholder?\n",
    "6. For what purpose(s)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ncnzPDQUq9JL"
   },
   "source": [
    "### Step 2: SP-LIME Global Explainer(for Tabular Data)\n",
    "\n",
    "As we just saw, explanations can vary a lot depending on what instance we pick. While this is great for explaining a single prediction, it makes it hard to give someone general intuition for \"how the model makes decision.\" That's where the submodular picker comes in. It picks useful, representative examples that together give global explanation for the model.\n",
    "\n",
    "The algorithm to do this is as follows:\n",
    "1. Calculate an explanation for all examples in the dataset\n",
    "2. Determine which features are important in explaining a lot of predictions -- that is, features that seem globally important\n",
    "3. Select (greedily) examples where the top globally important feature is part of the local explanation for that one example's prediction\n",
    "4. Continue selecting examples until we've covered as many of the globally important features as possible, constrained by the number of features that the user wants returned (num_exps_desired)\n",
    "\n",
    "You can read the details of how this is done in the LIME paper in BS."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zMga3Fkzq9JL"
   },
   "source": [
    "# Initialize the SP object\n",
    "sp_obj = submodular_pick.SubmodularPick(explainer, train.values, predict_fn, sample_size=10, \n",
    "                                        num_features=5, num_exps_desired=5)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ub4Zpf0q9JM"
   },
   "source": [
    "The attribute V tells us the best indices from the test set to explain the overall predictions of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fFWAZSh6q9JM"
   },
   "source": [
    "sp_obj.V"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8_dwJfBGq9JN"
   },
   "source": [
    "Now, we can get explantions for each of those examples:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "u9VwYBLwq9JN"
   },
   "source": [
    "for ind in sp_obj.V:\n",
    "    exp = explainer.explain_instance(test.values[ind], predict_fn, num_features=5)\n",
    "    print(\"Actual class: \", labels_test[ind])\n",
    "    exp.show_in_notebook(show_all=False)\n",
    "    print(\"==========================\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ktCjp5zq9Jr"
   },
   "source": [
    "**Questions:**\n",
    "\n",
    "Based on these (carefully chosen) examples, \n",
    "1. what can you say to someone who wanted to know how our model makes decisions?\n",
    "2. How are these explanaions useful for?\n",
    "3. For what purposes?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rR60m94XQJsQ"
   },
   "source": [
    "## Part III: SHAP: Post-Hoc Explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Step 1: Re-load the dataset and Re-Train the model (to fit SHAP input format)\n",
    "- Step 2: Create and Instentate a SHAP explainer (TreeSHAP)\n",
    "- Step 3: Bar chart of mean feature importance (Global Explanations of feature importance)\n",
    "- Step 4: Visualize a single prediction(s) (Local Explanation) \n",
    "- Step 5: Visualize many predictions (Local Overview) or Filtered predictions (Cohort)\n",
    "- Step 6: Summary Plot"
   ],
   "metadata": {
    "id": "VcuuvDJuvo_G"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rEx777D5wGMH"
   },
   "source": [
    "### Step 1: Re-Train the model (and re-load the dataset)\n",
    "We will retain the model to fit SHAP expected input format. We will download the same dataset (adult dataset) from the SHAP repositiry. The dataset containes the same data entries as the one downloaded from the original repository. But SHAP also offeres an encoded version of the data. We will use this for convinence sake but you can also work with the original data and encode your features using the tools available in sklearn library (for example).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Re-load dataset"
   ],
   "metadata": {
    "id": "vHKaB4iTJghV"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Load the adult data set from SHAP repository\n",
    "X,y = shap.datasets.adult()\n",
    "\n",
    "# Get the encoded dataset for SHAP repository \n",
    "X_display,y_display = shap.datasets.adult(display=True)\n"
   ],
   "metadata": {
    "id": "d7H2NWbmWRFS"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2xk1BJOZwGMI"
   },
   "outputs": [],
   "source": [
    "# create a train/test split\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.80, random_state=10)\n",
    "# train, test, labels_train, labels_test = sklearn.model_selection.train_test_split(X, y, train_size=0.80, random_state=10)\n",
    "print(\"Train shape: \", X_train.shape)\n",
    "print(\"Test shape: \", X_test.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-k70areBwGMI"
   },
   "source": [
    "Re-train the model (actually train a new model with the exact same features and parameters)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "b5LiOGgU1szb"
   },
   "source": [
    "# Fit the model\n",
    "gbtree_shap = xgboost.XGBClassifier(n_estimators=200, max_depth=5)\n",
    "gbtree_shap.fit(X_train, y_train)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check accuracy of the new/re-trained model."
   ],
   "metadata": {
    "id": "qlxhBJLT5wDo"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fkldV2fI1szd"
   },
   "source": [
    "# Make predictions \n",
    "pred_labels_test = gbtree_shap.predict(X_test)\n",
    "# Test accuracy again (it should be the same as the first model ~ 0.87 +/- 0.02)\n",
    "print(\"Test set accuracy: \", sklearn.metrics.accuracy_score(y_test, pred_labels_test))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0rytBS0PwGMK"
   },
   "source": [
    "###Step 2: Create and Instentate a SHAP explainer (TreeSHAP)\n",
    "\n",
    "Here we use the Tree SHAP implementation integrated into XGBoost to explain the entire dataset.\n",
    "\n",
    "- Global interpretable: SHAP provides model wide relevant explanations. \n",
    "- Can be used to produce local explanations (for specific data point(s)) \n",
    "- SHAP is Model-agnostic, however TreeSHAP is Model-Specific (Decision Trees)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ygDch8RiwGMK"
   },
   "outputs": [],
   "source": [
    "# this takes a minute or two since we are explaining over 30 thousand samples in a model with over a thousand trees\n",
    "explainer = shap.TreeExplainer(gbtree_shap)\n",
    "shap_values = explainer.shap_values(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CpB_7Eatus_x"
   },
   "source": [
    "### Step 3: Bar chart of mean feature importance (Global Explanation)\n",
    "\n",
    "This takes the average of the SHAP value magnitudes across the dataset and plots it as a simple bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g1W6tEvjus_x"
   },
   "outputs": [],
   "source": [
    "#Uncomment if using Colab\n",
    "shap.initjs()\n",
    "shap.summary_plot(shap_values, X_display, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The global mean(|Tree SHAP|) method applied to the income prediction model. The x-axis is essentially the average magnitude change in model output when a feature is “hidden” from the model (for this model the output has log-odds units). See [SHAP-related papers](https://github.com/slundberg/shap) for details, but “hidden” means integrating the variable out of the model. Since the impact of hiding a feature changes depending on what other features are also hidden, Shapley values are used to enforce consistency and accuracy."
   ],
   "metadata": {
    "id": "BNIGHGPCKE-6"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Questions:**\n"
   ],
   "metadata": {
    "id": "1dN_o-oEV84R"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0yV3glEwGML"
   },
   "source": [
    "### Step 4: Visualize a single prediction (Local Explanation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Waterfall plot**"
   ],
   "metadata": {
    "id": "LECN9qhjy8Vw"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "shap.initjs()\n",
    "\n",
    "#i is the index of the data point of intrest\n",
    "i=0\n",
    "_waterfall.waterfall_legacy(explainer.expected_value, shap_values[i,:], X_display.iloc[i,:])"
   ],
   "metadata": {
    "id": "Mas3KZoNcUXi"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**How to read the plot**\n",
    "\n",
    "The above explanation shows features each contributing to push the model output from the base value (the average model output over the training dataset we passed) to the model output. Features pushing the prediction higher are shown in red, those pushing the prediction lower are in blue"
   ],
   "metadata": {
    "id": "S_g1LyUQe9Jv"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "In the plot above, E\\[F(X)\\] is the model’s score for this observation. Higher scores lead the model to predict 1 and lower scores lead the model to predict 0. <br>\n",
    "Remember Label mapping:  {' <=50K': 0, ' >50K': 1}<br>\n",
    "0: the person earns 50K or Less/year (Red)<br>\n",
    "1: the person earns more than 50K /year (Blue)<br>\n",
    "\n",
    "E\\[F(X)\\]= -1.383 is the model baseline </br>\n",
    "\n",
    "The plot shows how features influence the prediction with respect to the baseline by tipping the score one way or another"
   ],
   "metadata": {
    "id": "0zPGMu3YjsqH"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**TO DO**\n",
    "1. Observe the explanation provided by TreeSHAP and compare it to the one Provided by LIME for the same datapoit \"i=0\"\n",
    "2. Change the datapoint of intrest and observe the new explanation (compare it to the same i value for LIME explanation)"
   ],
   "metadata": {
    "id": "hEJz5CIbf528"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# write the explanation code for the new data point here"
   ],
   "metadata": {
    "id": "2h0dq723f25N"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**\"String-like\" plot**\n",
    "\n"
   ],
   "metadata": {
    "id": "aTT_0Y-MfTwS"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uR6dkJBeus_o"
   },
   "outputs": [],
   "source": [
    "#Uncomment if using Colab\n",
    "shap.initjs()\n",
    "i=0\n",
    "shap.force_plot(explainer.expected_value, shap_values[i,:], X_display.iloc[i,:], matplotlib=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**TO DO**\n",
    "1. Change the datapoint of intrest and observe the new explanation "
   ],
   "metadata": {
    "id": "SX1zVp-gf5PS"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# write the explanation code for the new data point here"
   ],
   "metadata": {
    "id": "8MsCisoLf3vW"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Questions**\n",
    "\n",
    "1. What did we learn from these two plots. How do we interpret them ? \n",
    "2. What does it not tell us? what do red and green mean? What's on the x axis?\n",
    "4. When we changed the datapoint for which we what explanation, what happend? Are similar features important for all datapoints?\n",
    "5. Who can use these explanations? Which stakeholder?\n",
    "6. For what purpose(s)?"
   ],
   "metadata": {
    "id": "3ppON18Af4H7"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UT8CHT7kus_x"
   },
   "source": [
    "### Step 5: Visualize many predictions (Local Overview) or Filtered predictions (Cohort)\n",
    "\n",
    "**Note** To keep the plot readable in the browser we only visualize data points for 1,000 individuals but you can use more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GoACkpbZus_x"
   },
   "outputs": [],
   "source": [
    "#Uncomment if using Colab\n",
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value, shap_values[:1000,:], X_display.iloc[:1000,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QIpBeUWGus_x"
   },
   "source": [
    "### Step 6: SHAP Summary Plot\n",
    "\n",
    "Rather than use a typical feature importance bar chart, we use a density scatter plot of SHAP values for each feature to identify how much impact each feature has on the model output for individuals in the validation dataset. Features are sorted by the sum of the SHAP value magnitudes across all samples. It is interesting to note that the relationship feature has more total model impact than the captial gain feature, but for those samples where capital gain matters it has more impact than age. In other words, capital gain effects a few predictions by a large amount, while age effects all predictions by a smaller amount.\n",
    "\n",
    "Note that when the scatter points don't fit on a line they pile up to show density, and the color of each point represents the feature value of that individual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "meoEwJ02us_x"
   },
   "outputs": [],
   "source": [
    "#Uncomment if using Colab\n",
    "shap.initjs()\n",
    "shap.summary_plot(shap_values, X)\n",
    "\n",
    "# with Possibility to filter and/or create cohort plots\n",
    "# shap.summary_plot(shap_values[:1000,:], X.iloc[:1000,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tree SHAP explains the change in the margin output of the model. \n",
    "Every person (data point) has one dot on each row. The x position of the dot is the impact of that feature on the model’s prediction for the person, and the color of the dot represents the value of that feature for the person. Dots that don’t fit on the row pile up to show density (there are 32,561 customers in this example)."
   ],
   "metadata": {
    "id": "5kKL30ajRDrK"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The features are sorted by mean(|Tree SHAP|) and so we again see the relationship feature as the strongest predictor of making over \\$50K annually. By plotting the impact of a feature on every sample we can also see important outlier effects. For example, while capital gain is not the most important feature globally, it is by far the most important feature for a subset of customers. The coloring by feature value shows us patterns such as how being younger lowers your chance of making over $50K, while higher education increases your chance of making over \\$50K."
   ],
   "metadata": {
    "id": "WTVcxyNAiBSv"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ---------------- End of the Lab ----------------"
   ],
   "metadata": {
    "id": "wKoXXmfXyqzH"
   }
  }
 ]
}
