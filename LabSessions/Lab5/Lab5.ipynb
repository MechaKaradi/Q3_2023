{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b56f31da-b753-4d57-9d5c-aa869382e81f",
   "metadata": {},
   "source": [
    "# SEN163A - Responsible Data Analytics\n",
    "## Lab session 5: Predictive Analytics: Regression and Classification\n",
    "### Delft University of Technology\n",
    "### Q3 2022\n",
    "\n",
    "**Instructor**: Dr. Ir. Jacopo De Stefani - J.deStefani@tudelft.nl\n",
    "\n",
    "**TAs**: Antonio Sanchez Martin - A.SanchezMartin@student.tudelft.nl\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "Lab session aim to:\n",
    "- Show and reinforce how models and ideas presented in class are put to practice.\n",
    "- Help you gather hands-on machine learning skills.\n",
    "\n",
    "Lab sessions are:\n",
    "\n",
    "- Learning environments where you work with Jupyter notebooks and where you can get support from TAs and fellow students.\n",
    "- Not graded and do not have to be submitted.\n",
    "- A good preparation for the assignments (which are graded).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f654ba8d-7ccf-4014-9364-b974dd0c3f1f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Application: Predictive analytics of a health and insurance related data\n",
    "\n",
    "In this lab session, we will explore how to performe predicitive analytics to solve both a classification (predicting a categorical variable) and a regression (predicting a numerical variable) task. \n",
    "The classification case will be related to the prediction of the occurrence of a stroke, based on both physiological measurements as well as user features.\n",
    "The regression case, on the other hand, will be related to the prediction of health insurance costs, based on user features and behaviour.\n",
    "\n",
    "#### Learning objectives\n",
    "After completing the following exercises you will be able to:\n",
    "\n",
    "1. Apply common preprocessing techniques to prepare data for machine learning techniques: categorical preprocessing, imputation.\n",
    "2. Split the available dataset into a training set (for model fitting) and a testing set (for performance evaluation).\n",
    "3. Fit benchmark models to determine baseline performances on both a classification and regression case.\n",
    "4. Compute the most commonly applied performance measures for classification and regression tasks.\n",
    "5. Fit the most commonly applied machine learning predictive models for classification and regression tasks.\n",
    "6. Compare predictive models across different performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "243c302e-4aca-4a14-b28c-059dff58840a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "\n",
    "import seaborn\n",
    "import matplotlib\n",
    "\n",
    "seaborn.set_palette(\"Set2\")\n",
    "seaborn.color_palette(\"Set2\")\n",
    "\n",
    "#\n",
    "seaborn.set(rc={\"figure.figsize\":(15, 10),\n",
    "            'legend.title_fontsize' : 25,\n",
    "            'legend.fontsize' : 20,\n",
    "            'xtick.labelsize' : 20,\n",
    "            'ytick.labelsize' : 20,\n",
    "            'axes.labelsize' : 25})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f36b96cf-152f-4fe7-87f3-b379053bfc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seaborn.set_context('notebook')\n",
    "seaborn.set_context('paper')\n",
    "#seaborn.set_context('talk')\n",
    "#seaborn.set_context('poster')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cdd8da-d449-4299-a49a-66ad798c52dd",
   "metadata": {},
   "source": [
    "# Predictive Analytics - Classification example\n",
    "\n",
    "The classification task we will be tackling is based on the [Stroke Prediction Dataset](https://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset?select=healthcare-dataset-stroke-data.csv).\n",
    "\n",
    "In this case, we will use the available data to try to predict the occurrence of a stroke (`stroke` variable) as a function of the other variables.\n",
    "\n",
    "Before starting the modeling task, please have a look at the metadata about the [Stroke Prediction Dataset](https://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset?select=healthcare-dataset-stroke-data.csv), in order to better understand the meaning of the different variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc75f4f7-3397-406b-b1c3-1bed24c022b9",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "## Activity 1.1 - Descriptive analytics\n",
    "\n",
    "We are going to use the `pandas` library to perform some exploratory understanding of the data.\n",
    "\n",
    "1. Load the dataset `healthcare-dataset-stroke-data.csv` in the `stroke_df` variable\n",
    "2. Display the content of the `stroke_df` variable\n",
    "3. What are the type of the different columns? Use the knowledge from `pandas` to determine the type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dbfb57c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>hypertension</th>\n",
       "      <th>heart_disease</th>\n",
       "      <th>ever_married</th>\n",
       "      <th>work_type</th>\n",
       "      <th>Residence_type</th>\n",
       "      <th>avg_glucose_level</th>\n",
       "      <th>bmi</th>\n",
       "      <th>smoking_status</th>\n",
       "      <th>stroke</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9046</td>\n",
       "      <td>Male</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Private</td>\n",
       "      <td>Urban</td>\n",
       "      <td>228.69</td>\n",
       "      <td>36.6</td>\n",
       "      <td>formerly smoked</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51676</td>\n",
       "      <td>Female</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Self-employed</td>\n",
       "      <td>Rural</td>\n",
       "      <td>202.21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>never smoked</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31112</td>\n",
       "      <td>Male</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Private</td>\n",
       "      <td>Rural</td>\n",
       "      <td>105.92</td>\n",
       "      <td>32.5</td>\n",
       "      <td>never smoked</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60182</td>\n",
       "      <td>Female</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Private</td>\n",
       "      <td>Urban</td>\n",
       "      <td>171.23</td>\n",
       "      <td>34.4</td>\n",
       "      <td>smokes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1665</td>\n",
       "      <td>Female</td>\n",
       "      <td>79.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Self-employed</td>\n",
       "      <td>Rural</td>\n",
       "      <td>174.12</td>\n",
       "      <td>24.0</td>\n",
       "      <td>never smoked</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  gender   age  hypertension  heart_disease ever_married  \\\n",
       "0   9046    Male  67.0             0              1          Yes   \n",
       "1  51676  Female  61.0             0              0          Yes   \n",
       "2  31112    Male  80.0             0              1          Yes   \n",
       "3  60182  Female  49.0             0              0          Yes   \n",
       "4   1665  Female  79.0             1              0          Yes   \n",
       "\n",
       "       work_type Residence_type  avg_glucose_level   bmi   smoking_status  \\\n",
       "0        Private          Urban             228.69  36.6  formerly smoked   \n",
       "1  Self-employed          Rural             202.21   NaN     never smoked   \n",
       "2        Private          Rural             105.92  32.5     never smoked   \n",
       "3        Private          Urban             171.23  34.4           smokes   \n",
       "4  Self-employed          Rural             174.12  24.0     never smoked   \n",
       "\n",
       "   stroke  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset healthcare-dataset in the stroke_df variable\n",
    "stroke_df = pandas.read_csv(\"data\\\\healthcare-dataset-stroke-data.csv\")\n",
    "\n",
    "stroke_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c294cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5110 entries, 0 to 5109\n",
      "Data columns (total 12 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   id                 5110 non-null   int64  \n",
      " 1   gender             5110 non-null   object \n",
      " 2   age                5110 non-null   float64\n",
      " 3   hypertension       5110 non-null   int64  \n",
      " 4   heart_disease      5110 non-null   int64  \n",
      " 5   ever_married       5110 non-null   object \n",
      " 6   work_type          5110 non-null   object \n",
      " 7   Residence_type     5110 non-null   object \n",
      " 8   avg_glucose_level  5110 non-null   float64\n",
      " 9   bmi                4909 non-null   float64\n",
      " 10  smoking_status     5110 non-null   object \n",
      " 11  stroke             5110 non-null   int64  \n",
      "dtypes: float64(3), int64(4), object(5)\n",
      "memory usage: 479.2+ KB\n"
     ]
    }
   ],
   "source": [
    "stroke_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e0f9cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5110 entries, 0 to 5109\n",
      "Data columns (total 12 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   id                 5110 non-null   int64  \n",
      " 1   gender             5110 non-null   object \n",
      " 2   age                5110 non-null   int32  \n",
      " 3   hypertension       5110 non-null   bool   \n",
      " 4   heart_disease      5110 non-null   bool   \n",
      " 5   ever_married       5110 non-null   object \n",
      " 6   work_type          5110 non-null   object \n",
      " 7   Residence_type     5110 non-null   object \n",
      " 8   avg_glucose_level  5110 non-null   float64\n",
      " 9   bmi                4909 non-null   float64\n",
      " 10  smoking_status     5110 non-null   object \n",
      " 11  stroke             5110 non-null   bool   \n",
      "dtypes: bool(3), float64(2), int32(1), int64(1), object(5)\n",
      "memory usage: 354.4+ KB\n"
     ]
    }
   ],
   "source": [
    "# make the gender column categorical\n",
    "stroke_df['gender'].astype('category')\n",
    "\n",
    "# make the ever_married column categorical\n",
    "stroke_df['ever_married'].astype('category')\n",
    "\n",
    "# make the work_type column categorical\n",
    "stroke_df['work_type'].astype('category')\n",
    "\n",
    "# make the Residence_type column categorical\n",
    "stroke_df['Residence_type'].astype('category')\n",
    "\n",
    "# make the smoking_status column categorical\n",
    "stroke_df['smoking_status'].astype('category')\n",
    "\n",
    "# make the stroke column boolean\n",
    "stroke_df['stroke'] = stroke_df['stroke'].astype('bool')\n",
    "\n",
    "# make the hypertension column boolean\n",
    "stroke_df['hypertension'] = stroke_df['hypertension'].astype('bool')\n",
    "\n",
    "# make the heart_disease column boolean\n",
    "stroke_df['heart_disease'] = stroke_df['heart_disease'].astype('bool')\n",
    "\n",
    "# make the age column int\n",
    "stroke_df['age'] = stroke_df['age'].astype('int')\n",
    "\n",
    "stroke_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e020c06-ce16-4e3c-98cd-dd0e452a00e0",
   "metadata": {},
   "source": [
    "## Activity 1.2 - Diagnostic analytics\n",
    "\n",
    "A common problem in many datasets is missing data, usually indicated by N/A, NA, NaN, and extreme values (outliers).\n",
    "\n",
    "As a reminder, several ways exist to deal with incomplete or missing data, the most common being:\n",
    "\n",
    "![MissingData](figures/MissingData.png)\n",
    "\n",
    "**Source:** *Skarga-Bandurova, I., Biloborodova, T., & Dyachenko, Y. (2018). Strategy to managing mixed datasets with missing items. In Information Processing and Management of Uncertainty in Knowledge-Based Systems. Theory and Foundations: 17th International Conference, IPMU 2018, Cádiz, Spain, June 11-15, 2018, Proceedings, Part II 17 (pp. 608-620). Springer International Publishing.*\n",
    "\n",
    "\n",
    "1. Is there any column containing missing data in this dataset?\n",
    "2. If there are any, display the column(s) containing missing data.\n",
    "3. Count the number of missing values in the column(s) containing missing data.\n",
    "4. Analyze the missing values and their potential causes, and propose the most appropriate way to process them in order to have a dataset without missing values for the further steps.\n",
    "5. Produce a new dataset `stroke_noNA_df` containing no missing values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc70b44-1cfa-40bd-a3d4-0d79c8488eb1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Activity 1.3\n",
    "\n",
    "In order to apply a Machine Learning predictive model on the [Stroke Prediction Dataset](https://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset?select=healthcare-dataset-stroke-data.csv) that we had previously imported in the `stroke_df` variable, we need to perform the following operations:\n",
    "\n",
    "1. Impute missing values (Done in 1.2 by dropping/imputing the missing values)\n",
    "2. Split data into training and test using the `train_test_split` function (1.3)\n",
    "3. Transform categorical variables (1.4)\n",
    "\n",
    "**N.B.**: Please note that the transformation in categorical variables needs to be done after the split into training and test set in order to avoid information leakage (normally the testing set should not be seen by the model during its training phase).\n",
    "\n",
    "We are going to use the `scikit-learn` library to perform most of the split and transformation tasks.\n",
    "\n",
    "Here you need to:\n",
    "1. Divide the `stroke_noNA_df` dataset into two variables:\n",
    "- `X` containing the input variables\n",
    "- `Y` containing the target variable (`stroke`)\n",
    "2. Use the `train_test_split` function to obtain `X_train, X_test, Y_train, Y_test` with a 70% train - 30% test split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beff7d4-61ad-41b5-8e9e-63553f810700",
   "metadata": {},
   "source": [
    "## Activity 1.4\n",
    "\n",
    "Before inputting the data to a Machine Learning model, we need all the inputs to be numeric.\n",
    "In order to transform categorical data into numeric ones, three techniques exist (cf. https://www.kaggle.com/code/alexisbcook/categorical-variables):\n",
    "- Dropping Categorical variables\n",
    "- Ordinal Encoding: A categorical variable is replaced by a single numerical variable, where each category is mapped to a different, increasing integer value.\n",
    "- One-hot Encoding: A categorical variable with $n$ different categories is replaced by $n$ binary variables, each of them corresponding to a category. \n",
    "\n",
    "We are going to use the `scikit-learn` library to perform the transformation of the variables and to subsequently fit the models.\n",
    "\n",
    "1. Have a look at the documentation of the [Scikit-learn](https://scikit-learn.org/stable/index.html) library \n",
    "2. Have a look at the following [code](https://www.kaggle.com/code/alexisbcook/categorical-variables) to perform the transformation of categorical variables and create the following variables to store the processed data:\n",
    "    - Dropping Categorical variables: `drop_X_train` and `drop_X_test`\n",
    "    - Ordinal Encoding: `label_X_train` and `label_X_test`\n",
    "    - One-hot Encoding: `OH_X_train` and `OH_X_test`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfa6f39-bce8-4506-a99a-3e27f419a055",
   "metadata": {},
   "source": [
    "## Activity 1.5\n",
    "\n",
    "Finally, with the data cleaned of missing values, and with the categorical variable appropriately transformed we are able to fit some models using the `scikit-learn` library.\n",
    "\n",
    "As seen in Lecture 5 a starter, we will will be using a baseline for classification models: a [Naive Bayesian Model](https://scikit-learn.org/stable/modules/naive_bayes.html)\n",
    "\n",
    "1. Have a look at the documentation of the [Scikit-learn](https://scikit-learn.org/stable/index.html) library for the Naive Bayes model.\n",
    "2. Initialize the model\n",
    "3. Use the `fit` function to perform the training of the model on the training set\n",
    "4. Use the `predict` function to perform the prediction of the model on the test set\n",
    "5. Use the `accuracy_score, balanced_accuracy_score, f1_score` to compare the predictions with the actual values and obtain different performance metrics about the models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e64a8a-f692-4646-b78b-0ad0d1aa223c",
   "metadata": {},
   "source": [
    "## Activity 1.6\n",
    "\n",
    "Now that you are familiar with the pipeline of training, testing and evaluating one model, you can easily repeat the procedure for multiple models.\n",
    "\n",
    "1. Have a look at the documentation of the [Scikit-learn](https://scikit-learn.org/stable/index.html) library for other classification models:\n",
    "    - Logistic Regression\n",
    "    - Decision Trees\n",
    "    - Random Forest\n",
    "    - Gradient Boosting\n",
    "    - Artificial Neural Networks\n",
    "    - K-Nearest Neighbors\n",
    "2. For each model:\n",
    "    1. Initialize the model\n",
    "    2. Use the `fit` function to perform the training of the model on the training set\n",
    "    3. Use the `predict` function to perform the prediction of the model on the test set\n",
    "    4. Use the `accuracy_score, balanced_accuracy_score, f1_score` to compare the predictions with the actual values and obtain performance metrics about the models.\n",
    "    \n",
    "3. Create a dictionary/Data Frame in order to be able to compare the performance scores of the different models.\n",
    "    1. Are there any differences in the values of the metrics?\n",
    "    2. Why are these values different? Check the documentation to get to know more about the metrics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283a6370-fe3e-491d-9dad-c448f854f6f1",
   "metadata": {},
   "source": [
    "## Activity 1.7\n",
    "\n",
    "Congratulations! By now you should be able to train, test and evaluate multiple models on a classification task.\n",
    "\n",
    "1. Have a look at the documentation of the [Scikit-learn](https://scikit-learn.org/stable/index.html) library for the different parameters of other classification models.\n",
    "\n",
    "2. Analyze the impact of different changes in the predictive setup on the model:\n",
    "- Does the amount of data in the training set affect the predictive performance? Try to apply the procedure by varying the training-test proportion.\n",
    "- Does the parameter setting of the different models have an impact on the model performances? Try to tweak the performance by varying the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36132f00-8f90-41c0-9657-1f34688dc595",
   "metadata": {},
   "source": [
    "# Predictive Analytics - Regression\n",
    "\n",
    "The regression task we will be tackling is based on the [Medical Cost Personal Dataset](https://www.kaggle.com/datasets/mirichoi0218/insurance?ref=hackernoon.com&select=insurance.csv).\n",
    "\n",
    "In this case, we will use the available data to try to predict the insurance cost (`charges` variable) as a function of the other variables.\n",
    "\n",
    "Before starting the modeling task, please have a look at the metadata about the [Medical Cost Personal Dataset](https://www.kaggle.com/datasets/mirichoi0218/insurance?ref=hackernoon.com&select=insurance.csv), in order to better understand the meaning of the different variables.\n",
    "\n",
    "## Activity 2.1 - Descriptive analytics\n",
    "\n",
    "We are going to use the `pandas` library to perform some exploratory understanding of the data.\n",
    "\n",
    "1. Load the dataset in the `insurance_df` variable\n",
    "2. Display the content of the `insurance_df` variable\n",
    "3. What are the type of the different columns? Use the knowledge from `pandas` to determine the type.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f41a5b-8f72-46eb-87c1-bb3e8562d1db",
   "metadata": {},
   "source": [
    "## Activity 2.2 - Diagnostic analytics\n",
    "\n",
    "A common problem in many datasets is missing data, usually indicated by N/A, NA, NaN, and extreme values (outliers).\n",
    "\n",
    "As a reminder, several ways exist to deal with incomplete or missing data, the most common being:\n",
    "\n",
    "![MissingData](figures/MissingData.png)\n",
    "\n",
    "**Source:** *Skarga-Bandurova, I., Biloborodova, T., & Dyachenko, Y. (2018). Strategy to managing mixed datasets with missing items. In Information Processing and Management of Uncertainty in Knowledge-Based Systems. Theory and Foundations: 17th International Conference, IPMU 2018, Cádiz, Spain, June 11-15, 2018, Proceedings, Part II 17 (pp. 608-620). Springer International Publishing.*\n",
    "\n",
    "\n",
    "1. Is there any column containing missing data in this dataset?\n",
    "2. If there are any, display the column(s) containing missing data.\n",
    "3. Count the number of missing values in the column(s) containing missing data.\n",
    "4. Analyze the missing values and their potential causes, and propose the most appropriate way to process them in order to have a dataset without missing values for the further steps.\n",
    "5. Produce a new dataset `insurance_noNA_df` containing no missing values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d98c7d-ae75-491f-8343-6aaa2a5ca926",
   "metadata": {},
   "source": [
    "## Activity 2.3\n",
    "\n",
    "In order to apply a Machine Learning predictive model on the [Stroke Prediction Dataset](https://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset?select=healthcare-dataset-stroke-data.csv) that we had previously imported in the `stroke_df` variable, we need to perform the following operations:\n",
    "\n",
    "1. Impute missing values (Done in 2.2 by dropping/imputing the missing values)\n",
    "2. Split data into training and test using the `train_test_split` function (2.3)\n",
    "3. Transform categorical variables (2.4)\n",
    "\n",
    "**N.B.**: Please note that the transformation in categorical variables needs to be done after the split into training and test set in order to avoid information leakage (normally the testing set should not be seen by the model during its training phase).\n",
    "\n",
    "We are going to use the `scikit-learn` library to perform most of the split and transformation tasks.\n",
    "\n",
    "Here you need to:\n",
    "1. Divide the `insurance_noNA_df` dataset into two variables:\n",
    "- `X` containing the input variables\n",
    "- `Y` containing the target variable (`charges`)\n",
    "2. Use the `train_test_split` function to obtain `X_train, X_test, Y_train, Y_test` with a 70% train - 30% test split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc2006e-5385-4e4c-8213-b4410b3f00e6",
   "metadata": {},
   "source": [
    "## Activity 2.4\n",
    "\n",
    "Before inputting the data to a Machine Learning model, we need all the inputs to be numeric.\n",
    "In order to transform categorical data into numeric ones, three techniques exist (cf. https://www.kaggle.com/code/alexisbcook/categorical-variables):\n",
    "- Dropping Categorical variables\n",
    "- Ordinal Encoding: A categorical variable is replaced by a single numerical variable, where each category is mapped to a different, increasing integer value.\n",
    "- One-hot Encoding: A categorical variable with $n$ different categories is replaced by $n$ binary variables, each of them corresponding to a category. \n",
    "\n",
    "We are going to use the `scikit-learn` library to perform the transformation of the variables and to subsequently fit the models.\n",
    "\n",
    "1. Have a look at the documentation of the [Scikit-learn](https://scikit-learn.org/stable/index.html) library \n",
    "2. Have a look at the following [code](https://www.kaggle.com/code/alexisbcook/categorical-variables) to perform the transformation of categorical variables and create the following variables to store the processed data:\n",
    "    - Dropping Categorical variables: `drop_X_train` and `drop_X_test`\n",
    "    - Ordinal Encoding: `label_X_train` and `label_X_test`\n",
    "    - One-hot Encoding: `OH_X_train` and `OH_X_test`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c076fe77-7c4d-419a-a30e-19cf6b381abe",
   "metadata": {},
   "source": [
    "## Activity 2.5\n",
    "\n",
    "Finally, with the data cleaned of missing values, and with the categorical variables appropriately transformed we are able to fit some models using the `scikit-learn` library.\n",
    "\n",
    "As seen in Lecture 5 a starter, we will will be using a baseline for regression models: a [Linear Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression)\n",
    "\n",
    "1. Have a look at the documentation of the [Scikit-learn](https://scikit-learn.org/stable/index.html) library for the Linear Regression model.\n",
    "2. Initialize the model\n",
    "3. Use the `fit` function to perform the training of the model on the training set.\n",
    "4. Use the `predict` function to perform the prediction of the model on the test set.\n",
    "5. Use the `mean_squared_error, mean_absolute_error, mean_absolute_percentage_error` to compare the predictions with the actual values and obtain different performance metrics about the models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a0ed3f-7a4a-4431-a0b5-6530a2d50872",
   "metadata": {},
   "source": [
    "## Activity 2.6\n",
    "\n",
    "Now that you are familiar with the pipeline of training, testing and evaluating one model, you can easily repeat the procedure for multiple models.\n",
    "\n",
    "1. Have a look at the documentation of the [Scikit-learn](https://scikit-learn.org/stable/index.html) library for other regression models:\n",
    "    - Perceptron\n",
    "    - Lasso/ElasticNet\n",
    "    - Decision Tree\n",
    "    - Random Forest\n",
    "    - Gradient Boosting\n",
    "    - Artificial Neural Networks\n",
    "    - K-Nearest Neighbors\n",
    "2. For each model:\n",
    "    1. Initialize the model\n",
    "    2. Use the `fit` function to perform the training of the model on the training set\n",
    "    3. Use the `predict` function to perform the prediction of the model on the test set\n",
    "    4. Use the `accuracy_score, balanced_accuracy_score, f1_score` to compare the predictions with the actual values and obtain performance metrics about the models.\n",
    "    \n",
    "3. Create a dictionary/Data Frame in order to be able to compare the performance scores of the different models.\n",
    "    1. Are there any differences in the values of the metrics?\n",
    "    2. Why are these values different? Check the documentation to get to know more about the metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec2ce23-4404-46f1-9afe-fd62b38c948d",
   "metadata": {},
   "source": [
    "## Activity 2.7\n",
    "\n",
    "Congratulations! By now you should be able to train, test and evaluate multiple models on a classification task.\n",
    "\n",
    "1. Have a look at the documentation of the [Scikit-learn](https://scikit-learn.org/stable/index.html) library for the different parameters of other classification models.\n",
    "\n",
    "2. Analyze the impact of different changes in the predictive setup on the model:\n",
    "- Does the amount of data in the training set affect the predictive performance? Try to apply the procedure by varying the training-test proportion.\n",
    "- Does the parameter setting of the different models have an impact on the model performances? Try to tweak the performance by varying the parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
