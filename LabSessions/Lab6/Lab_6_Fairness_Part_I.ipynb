{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 6 - Operational Fairness in DA/ML\n",
        "Week 6 - Q3, 22/23 <br>\n",
        "SEN163B: Responsible Data Analytics<br>\n",
        " "
      ],
      "metadata": {
        "id": "6g6YXEhab45J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By <b> Nadia Metoui* </b> <br>\n",
        "TA <b> Darsh Modi</b> <br> \n",
        "Faculty of Technology, Policy, and Management (TPM)<br>"
      ],
      "metadata": {
        "id": "pSJOyGv7cFgL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Learning Objectives***<br>\n",
        "At the end of this lab, you will be able to \n",
        "\n",
        "- Use data analytics tools to measure disaparities in a Data Analytics Project\n",
        "- Use data analytics tools to mitigate disaparities in a Data Analytics Project\n"
      ],
      "metadata": {
        "id": "D517eJw7jtwU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Structure***\n",
        "- Part I. Measuring Disparities with Aequitas\n",
        "- Part II Mitigating Bias/Disparities with FAI360 (will be provided in a separate notebook for the second part of the Lab)\n",
        "- Part III: Mitigating Bias/Disparities with FairLearn (a tutorial will be provided for homework exploration)\n"
      ],
      "metadata": {
        "id": "d0t8WNfupsvU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Part I: Measuring Disparities with Aequitas*"
      ],
      "metadata": {
        "id": "PW9WOVdukAgA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "In this first part of the lab, we will explore [**Aequitas**](http://www.datasciencepublicpolicy.org/our-work/tools-guides/aequitas/), Bias and fairness auditing toolkit with a python library implementation. Aequitas does NOT have any mitigation tools. It only detects bias, defined as disaparities in data. It gives very good visualizations you can use to highlight the importance of disaparities to your data team.\n",
        "\n",
        "\n",
        "We will also explore **Intersectionality**, and reflect on its effects.\n",
        "\n",
        "Then, we will explore **The Fairness Tree**, a flow chart created by the [Data Science and Public Policy Lab at CMU](http://www.datasciencepublicpolicy.org/) to help find the fairness measures relevant to your own responsible data analytics project, when you do don't forget to think about different stakeholders.\n",
        "\n",
        "---\n",
        "Acknowledgement: this Part of  lab is inspired from the offical Aequitas tutorial. You can find the tutorial [*HERE*](https://dssg.github.io/aequitas/examples/compas_demo.html). Take a look and use some of the code to help you.\n",
        "\n",
        "The Official publication on the Aequitas toolkit can be found [*HERE*](https://arxiv.org/pdf/1811.05577.pdf). It is also on Brightspace. Read it! it is very relevant to your project."
      ],
      "metadata": {
        "id": "Aaht5r74IwjI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analysis Steps\n",
        "- Step 0: Understanding the Use Case (COMPAS)\n",
        "- Step 1: Set-up \n",
        "- Step 2: Explore and familiarize with the data\n",
        "- Step 3: Explore Aequitas Toolkit"
      ],
      "metadata": {
        "id": "zCAFmNHE0TOA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 0: Understanding the Use Case (COMPAS)"
      ],
      "metadata": {
        "id": "99NDX9CRW6su"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In 2016, ProPublica reported on racial inequality in automated criminal risk assessment algorithms. The [report](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing) is based on [this analysis](https://github.com/propublica/compas-analysis). Using a clean version of the COMPAS dataset from the ProPublica GitHub repo, we demostrate the use of the Aequitas bias reporting tool.\n",
        "\n",
        "Northpointe's COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) is one of the most widesly utilized risk assessment tools/ algorithms within the criminal justice system for guiding decisions such as how to set bail. The ProPublica dataset represents two years of COMPAS predicitons from Broward County, FL."
      ],
      "metadata": {
        "id": "GgoKjTtwseGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "COMPAS produces a risk score that predicts a person's likelihood of commiting a crime in the next two years. The output is a score between 1 and 10 that maps to low, medium or high. For Aequitas, we collapse this to a binary prediction. A score of 0 indicates a prediction of \"low\" risk according to COMPAS, while a 1 indicates \"high\" or \"medium\" risk.\n",
        "\n",
        "This categorization is based on ProPublica's interpretation of Northpointe's practioner guide:\n",
        "\n",
        "    \"According to Northpointe’s practitioners guide, COMPAS “scores in the medium and high range \n",
        "    garner more interest from supervision agencies than low scores, as a low score would suggest \n",
        "    there is little risk of general recidivism,” so we considered scores any higher than “low” to \n",
        "    indicate a risk of recidivism.\""
      ],
      "metadata": {
        "id": "fwPmmhUVsskF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 1: Set-up"
      ],
      "metadata": {
        "id": "Kcq1Rt1k0xv1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will first install the library *aequitas* "
      ],
      "metadata": {
        "id": "aSS_IJiW0Cjl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-danger\">\n",
        "<b>Note:</b> Uncomment and run the next cell if you have not previously installed aequitas.\n",
        "</div>"
      ],
      "metadata": {
        "id": "yiUYWqW71lZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install aequitas"
      ],
      "metadata": {
        "id": "sAibV3qh0Ctu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#in case you have Attributeerror: module ‘pil.image’ has no attribute ‘resampling’!\n",
        "!pip install Pillow==9.1.0 "
      ],
      "metadata": {
        "id": "AoiYBJfa99dQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then you will load  the required libraries for this part (usually no installation is needed).  The main libraries we will use in Part I are `pandas`, `seaborn` and `aequitas`. "
      ],
      "metadata": {
        "id": "dUFohi800DDD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from aequitas.group import Group\n",
        "from aequitas.bias import Bias\n",
        "from aequitas.fairness import Fairness\n",
        "import aequitas.plot as ap\n",
        "\n",
        "# import warnings; warnings.simplefilter('ignore')\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "6XgC7O_p0DLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 2: Explore and familiarize with the data"
      ],
      "metadata": {
        "id": "USlAUlSm0y0I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load and undestaind the data**"
      ],
      "metadata": {
        "id": "UpBAhGzj1_68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"https://github.com/dssg/aequitas/raw/master/examples/data/compas_for_aequitas.csv\")\n"
      ],
      "metadata": {
        "id": "-nQNnr8z2AEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data provided for this Part of the Lab is not the \"full\" training data used for COMPAS. It is a dataset composed of: sensitive attributes used to train COMPAS `race`,\t`sex`,\t`age_cat`, The original labels used to train COMPAS, and the output score or predictions of COMPAS.\n",
        "\n",
        "COMPAS uses a logistic regression-based model to predict a recidivism score. We will not concern ourselves with training the model in this part. We will only use the output data after training and testing\n",
        "\n",
        "\n",
        "For your project, you will build such a table before using Aequitas. To do so you will use only your test data. You select the sensitive attributes and their proxies the ground-truth labels and the predictions made by your model\n",
        "Aequitas is picky about the names of columns given to it. You have to provide the following columns:\n",
        "1. `score` is the prediction made by your model\n",
        "1. `label_value` is the ground-truth label\n",
        "1. The only other columns you should keep are sensitive attribute (and proxy) labels\n",
        "\n",
        "**Note:** remember Aequitas only uses binary scores so if you are using regressions you have to collapse predictions to a binary setting. Here a score of 0 indicates a prediction of a \"low\" risk of recidivism 1 indicates a \"high\" or \"medium\" risk."
      ],
      "metadata": {
        "id": "IoVbWtnO5Vlm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "2OXcx-7-2AXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explore the data**\n",
        " "
      ],
      "metadata": {
        "id": "-HXkcT4O2AN4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Q1: Explore and understaind the data</b>\n",
        "\n",
        "Produce as many visualisation and table exploration as you wish. The goal is to get a sence of the data destributions. E.g., \n",
        "- What are the attributs and featuers?\n",
        "- What is the score destribution by attribute (race, sex or age) etc...\n",
        "\n",
        "**Tip**: You can use `sns.countplot()`"
      ],
      "metadata": {
        "id": "3YuL8PhI2OCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add your code here"
      ],
      "metadata": {
        "id": "D30PWUuz2OQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add your code here and add any number of cells you need"
      ],
      "metadata": {
        "id": "HjExRbd72OVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aq_palette = sns.color_palette(\"Set1\", 2)\n",
        "by_race = sns.countplot(x=\"race\", hue=\"score\", data=df[df.race.isin(['African-American', 'Caucasian', 'Hispanic'])], palette=aq_palette)"
      ],
      "metadata": {
        "id": "x-OMkrWp9Ptb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "by_sex = sns.countplot(x=\"sex\", hue=\"score\", data=df, palette=aq_palette)"
      ],
      "metadata": {
        "id": "z3mBtNW89P1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Q2: Could you identify any disparities in these destributions</b>"
      ],
      "metadata": {
        "id": "cRRyFt0U9DOC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Discuss the disparities e.g., based on sex, race or age category"
      ],
      "metadata": {
        "id": "0KaR4LJk-dx_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add your answer here"
      ],
      "metadata": {
        "id": "ysVzROWa-mtV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 3: Explore Aequitas Toolkit"
      ],
      "metadata": {
        "id": "at6K8aJl0zDW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point if your exploration you should have identified patternes of disparities or destribution skew in COMPAS output predictions\n",
        "\n",
        "As a data scientist you face the challenge of determining whether or not such patterns reflect bias/unfairness or not. The fact that there are multiple ways to measure bias adds complexity to the decision-making process. With Aequitas, You can build a report of various fairness metrics to aid in this process.\n",
        "\n",
        "Applying Aequitas progammatically is a three step process represented by three python classes: \n",
        "\n",
        "`Group()`: Define groups \n",
        "\n",
        "`Bias()`: Calculate disparities\n",
        "\n",
        "`Fairness()`: Assert fairness\n",
        "\n",
        "Each class builds on the previous one expanding the output DataFrame."
      ],
      "metadata": {
        "id": "jeVzvIrV8-FX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing and Data Formatting**\n",
        "\n",
        "as mentioned above Aequitas The Aequitas tool always requires a binary **`score`** column and a binary **`label_value`** these will be used to generate metrics such as False Discovery Rate, False Positive Rate, False Omission Rate, and False Negative Rate).\n",
        "\n",
        "Preprocessing includes but is not limited to checking for mandatory `score` and `label_value` columns as well as at least one column representing sensitive attributes specific to the dataset. See [documentation](../input_data.html) for more information about input data.\n",
        "\n",
        "**Note** `entity_id` is not necessary for this example, Aequitas recognizes `entity_id` as a reserve column name and will not recognize it as an attribute column."
      ],
      "metadata": {
        "id": "69k1nYFFBfzD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*** Explore Biases exist in COMPAS model***\n",
        "\n",
        "_Aequitas Group() Class_\n",
        "\n",
        "Aequitas's `Group()` class enables researchers to evaluate biases across all subgroups in their dataset by assembling a confusion matrix of each subgroup, calculating commonly used metrics such as false positive rate and false omission rate, as well as counts by group and group prevelance among the sample population. "
      ],
      "metadata": {
        "id": "bvmMiVMlOEqa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id='counts_description'></a>\n",
        "The **`get_crosstabs()`** method tabulates a confusion matrix for each subgroup and calculates commonly used metrics such as false positive rate and false omission rate. It also provides counts by group and group prevelances.\n",
        "\n",
        "#### Group Counts Calculated:\n",
        "\n",
        "| Count Type | Column Name |\n",
        "| --- | --- |\n",
        "| False Positive Count | 'fp' |\n",
        "| False Negative Count | 'fn' |\n",
        "| True Negative Count | 'tn' |\n",
        "| True Positive Count | 'tp' |\n",
        "| Predicted Positive Count | 'pp' |\n",
        "| Predicted Negative Count | 'pn' |\n",
        "| Count of Negative Labels in Group | 'group_label_neg' |\n",
        "| Count of Positive Labels in Group | 'group_label_pos' | \n",
        "| Group Size | 'group_size'|\n",
        "| Total Entities | 'total_entities' |\n",
        "\n",
        "#### Absolute Metrics Calculated:\n",
        "\n",
        "| Metric | Column Name |\n",
        "| --- | --- |\n",
        "| True Positive Rate | 'tpr' |\n",
        "| True Negative Rate | 'tnr' |\n",
        "| False Omission Rate | 'for' |\n",
        "| False Discovery Rate | 'fdr' |\n",
        "| False Positive Rate | 'fpr' |\n",
        "| False Negative Rate | 'fnr' |\n",
        "| Negative Predictive Value | 'npv' |\n",
        "| Precision | 'precision' |\n",
        "| Predicted Positive Ratio$_k$ | 'ppr' |\n",
        "| Predicted Positive Ratio$_g$ | 'pprev' |\n",
        "| Group Prevalence | 'prev' |\n"
      ],
      "metadata": {
        "id": "pKCWU1jqTW0m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3: Use `get_crosstabs()` method expects a dataframe with predefined columns `score`, and `label_value` and treats other columns (with a few exceptions) as attributes against which to test for disparities. In this case, we include `race`, `sex` and `age_cat`. **"
      ],
      "metadata": {
        "id": "063wrFXATaC4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**: The **`get_crosstabs()`** method expects a dataframe with predefined columns `score`, and `label_value` and treats other columns (with a few exceptions) as attributes against which to test for disparities. In this case, we include `race`, `sex` and `age_cat`. "
      ],
      "metadata": {
        "id": "2CoFJ6FvTaL9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5j72P8XsQDZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4: What are bias metrics across groups?**\n",
        "\n",
        "Once you have run the `Group()` class **`get_crosstabs()`** method, you'll have a dataframe of the [group counts](#counts_description) and [group value bias metrics](#counts_description).\n",
        "\n",
        "The `Group()` class has a **`list_absolute_metrics()`** method, which you can use for faster slicing to view just  counts or bias metrics.\n",
        "\n",
        "A. Display counts across sample population groups\n",
        "\n",
        "B. calculated absolute metrics for each sample population group\n",
        "\n",
        "C. Comment the results"
      ],
      "metadata": {
        "id": "kUrHpcg6Uplw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AnYsmVzhUpt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1H3Tf-gqlY6"
      },
      "source": [
        "*** How do I interpret biases in my model? ***\n",
        "\n",
        "In the slice of the crosstab dataframe created by the `Group()` class **`get_crosstabs()`** method directly above, we see that African-Americans have a false positive rate (`fpr`) of 45%, while Caucasians have a false positive rate of only 23%. This means that African-American people are far more likely to be falsely labeled as high-risk than white people. On the other hand, false ommision rates (`for`) and false discovery rates (`fdr`) are much closer for those two groups."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzvhRn1oqlY6"
      },
      "source": [
        "\n",
        "## What levels of disparity exist between population groups?\n",
        "\n",
        "### _Aequitas Bias() Class_\n",
        "We use the Aequitas `Bias()` class to calculate disparities between groups based on the crosstab returned by the `Group()` class **`get_crosstabs()`** method described above. Disparities are calculated as a ratio of a metric for a group of interest compared to a base group. For example, the False Negative Rate Disparity for black defendants vis-a-vis whites is:\n",
        "$$Disparity_{FNR} =  \\frac{FNR_{black}}{FNR_{white}}$$ \n",
        "\n",
        "Below, we use **`get_disparity_predefined_groups()`** which allows us to choose reference groups that clarify the output for the practitioner. \n",
        "\n",
        "The Aequitas `Bias()` class includes two additional get disparity functions: **`get_disparity_major_group()`** and **`get_disparity_min_metric()`**, which automate base group selection based on sample majority (across each attribute) and minimum value for each calculated bias metric, respectively.  \n",
        "\n",
        "The **`get_disparity_predefined_groups()`** allows user to define a base group for each attribute, as illustrated below. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qBiXBiIqlY6"
      },
      "source": [
        "#### Disparities Calculated Calcuated:\n",
        "\n",
        "| Metric | Column Name |\n",
        "| --- | --- |\n",
        "| True Positive Rate Disparity | 'tpr_disprity' |\n",
        "| True Negative Rate | 'tnr_disparity' |\n",
        "| False Omission Rate | 'for_disparity' |\n",
        "| False Discovery Rate | 'fdr_disparity' |\n",
        "| False Positive Rate | 'fpr_disparity' |\n",
        "| False NegativeRate | 'fnr_disparity' |\n",
        "| Negative Predictive Value | 'npv_disparity' |\n",
        "| Precision Disparity | 'precision_disparity' |\n",
        "| Predicted Positive Ratio$_k$ Disparity | 'ppr_disparity' |\n",
        "| Predicted Positive Ratio$_g$ Disparity | 'pprev_disparity' |\n",
        "\n",
        "\n",
        "Columns for each disparity are appended to the crosstab dataframe, along with a column indicating the reference group for each calculated metric (denoted by `[METRIC NAME]_ref_group_value`). We see a slice of the dataframe with calculated metrics in the next section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-18T18:07:38.204989Z",
          "start_time": "2020-08-18T18:07:38.200478Z"
        },
        "id": "TLtaR5wQqlY6"
      },
      "outputs": [],
      "source": [
        "b = Bias()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GB9QA6icqlY6"
      },
      "source": [
        "***Q5: write the code to calculate disparities in relation to a user-specified group for each attribute using `get_disparity_predefined_groups`***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-18T18:07:39.798714Z",
          "start_time": "2020-08-18T18:07:38.214052Z"
        },
        "id": "rg-zhZuOqlY7"
      },
      "outputs": [],
      "source": [
        "#code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeUNvhTGqlY7"
      },
      "source": [
        "The `Bias()` class includes a method to quickly return a list of calculated disparities from the dataframe returned by the **`get_disparity_`** methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-18T18:07:39.878536Z",
          "start_time": "2020-08-18T18:07:39.805271Z"
        },
        "id": "RgXJP1G-qlY7"
      },
      "outputs": [],
      "source": [
        "Q6: Add disparity metrics to the dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sj6b6UnZqlY7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad_eGZd5qlY7"
      },
      "source": [
        "***Q7: Aequitas Visualizations***\n",
        "Use `summary` to displya the over all fairness of the COMPAS model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7KPMboWqlY7"
      },
      "outputs": [],
      "source": [
        "# your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_S8snFMoqlY7"
      },
      "source": [
        "***Q8: Check for disparities in Race***\n",
        "\n",
        "Use `disparity` to display disparities by sensitive attribut  `Race`, `Sex` and `Age`"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RXVva0plXCRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q 9: Intersectionality**\n",
        "\n",
        "A. Starting from the original dataframe (uploaded at the beginning of the lab), write code to create a new column in `data` called `race_sex` which concatenates the `race` and `sex` columns with an underscore, so it has entries like `African-American-Male` or `Hispanic_Female`, etc.\n",
        "\n",
        "B. List the new intersectional categories"
      ],
      "metadata": {
        "id": "Xi4OCr1-QDiX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A. add code"
      ],
      "metadata": {
        "id": "rmHYuqrSQ3co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add code"
      ],
      "metadata": {
        "id": "43gAjhB4Q3fq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "B. Your answer"
      ],
      "metadata": {
        "id": "j6Y1AlWnRTq4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q 10: Bias in Intersectionality**\n",
        "\n",
        "A. Repeat the analysis above focusing in this new intersectional categories. \n",
        "\n",
        "B. Write down your observations."
      ],
      "metadata": {
        "id": "AXirlBmEQ4Dd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add code"
      ],
      "metadata": {
        "id": "8061Iy1sRXE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "geMpAmH1RXMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MwZYgLcmRXV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***How do we understand biases in our model?***\n",
        "\n",
        "What metrics are more relevant to your scenario?\n",
        "\n",
        "One of the major contributions of the Aequitas toolkit and enviornment is the `Fairness Tree`.\n",
        "\n",
        "While a lot of bias metrics and fairness definitions have been proposed, there is no consensus on which definitions and metrics should be used in practice to evaluate and audit these systems. To help with this, the authors of Aequitas also created a guide for understanding when and what metrics might apply in a given situation, called the `Fairness Tree`. It can be found here: http://www.datasciencepublicpolicy.org/our-work/tools-guides/aequitas/\n",
        "\n"
      ],
      "metadata": {
        "id": "L3CMXQSwIwjQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider the following 2 scenarios:\n",
        "\n",
        "- **Scenario 1**\n",
        "You are using the COMPAS model as described at the beginning of the Lab. The carseral system will use COMPAS to optimise the organisation of hearings for parol and early release. The output of your model is going to be used to determine if you grant a hearing requeste for convicts early release or if you reject these requestes. Low risk scors will be grated a hearing for early release. High and Medium scores will not be granted a hearing \n",
        "\n",
        "- **Scenario 2**\n",
        "Suppose a certain country in Europe is building a model to predict the risk that a criminal is going to recidivate, similiar to the prediction made by the COMPAS model discussed in class.  However, in contrast to COMPAS, the goal is not to use these scores to determine whether or not an individual is allowed to be free; instead, almost all individuals with a high risk score will be admitted into a special program that has three components:  (1) the individuals recieve one-on-one counseling, (2) the individuals will recieve a monthly stipend, and (3) the individuals are set up with housing.  The program is a benefit to the individuals and is aimed at reducing recividism.\n",
        "\n",
        "Think about the confusion matrix for the model.  How would you define the positive and negative class?\n",
        "\n",
        "Suppose the country is trying to decide on the proper fairness metric to use for their machine learning model, and cares about the protected attribute race.  In terms of representation, they would either accept a model that has equal nominal representation of different races, or equal proportional representation in the special program.  They are also concerned about errors made by the model, and want to make sure that predictive equity among groups is acheieved for people with need.\n",
        "\n",
        "Review the Aequitas guidelines for fairness metrics, and think about them in the context of this problem:  http://www.datasciencepublicpolicy.org/projects/aequitas/.\n",
        "\n",
        "\n",
        "**Q:** Which of the following is or is not a good choice for a fairness metric for the model in each of the scenarios?\n",
        "\n",
        "> a) False Negative Rate Parity \\\n",
        "> b) False Positive Rate Parity \\\n",
        "> c) Equal Selection Parity \\\n",
        "> d) Demographic Parity \\\n",
        "\n",
        "Justify your answers and compare the two scenarios"
      ],
      "metadata": {
        "id": "k3lP9kf6IwjQ"
      }
    }
  ],
  "metadata": {
    "orig_nbformat": 4,
    "language_info": {
      "name": "python",
      "version": "3.9.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.5 64-bit ('base': conda)"
    },
    "interpreter": {
      "hash": "569d6b7e9215e11aba41c6454007e5c1b78bad7df09dab765d8cf00362c40f03"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}